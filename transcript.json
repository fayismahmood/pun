[
  {
    "text": "hi this is jeffy and welcome to",
    "duration": 2.72,
    "offset": 0.32,
    "lang": "en"
  },
  {
    "text": "applications of deep neural networks",
    "duration": 4.24,
    "offset": 1.68,
    "lang": "en"
  },
  {
    "text": "with washington university in this video",
    "duration": 5.44,
    "offset": 3.04,
    "lang": "en"
  },
  {
    "text": "we&amp;#39;re going to see how we can actually",
    "duration": 4.639,
    "offset": 5.92,
    "lang": "en"
  },
  {
    "text": "train a hugging face neural network",
    "duration": 3.84,
    "offset": 8.48,
    "lang": "en"
  },
  {
    "text": "fine-tune it further from the",
    "duration": 4.24,
    "offset": 10.559,
    "lang": "en"
  },
  {
    "text": "pre-trained weights that it has using a",
    "duration": 5.76,
    "offset": 12.32,
    "lang": "en"
  },
  {
    "text": "data set on hugging face as well as a",
    "duration": 5.821,
    "offset": 14.799,
    "lang": "en"
  },
  {
    "text": "pre-trained model on hugging face",
    "duration": 7.94,
    "offset": 18.08,
    "lang": "en"
  },
  {
    "text": "[Music]",
    "duration": 5.4,
    "offset": 20.62,
    "lang": "en"
  },
  {
    "text": "so now we&amp;#39;re going to actually train a",
    "duration": 4.56,
    "offset": 27.119,
    "lang": "en"
  },
  {
    "text": "model and hugging face i&amp;#39;m going to go",
    "duration": 5.599,
    "offset": 28.8,
    "lang": "en"
  },
  {
    "text": "ahead and open this in collab pro you",
    "duration": 4.72,
    "offset": 31.679,
    "lang": "en"
  },
  {
    "text": "don&amp;#39;t really need colab pro i&amp;#39;m using",
    "duration": 4.241,
    "offset": 34.399,
    "lang": "en"
  },
  {
    "text": "co-ed pro plus it works just fine in the",
    "duration": 3.441,
    "offset": 36.399,
    "lang": "en"
  },
  {
    "text": "free",
    "duration": 3.28,
    "offset": 38.64,
    "lang": "en"
  },
  {
    "text": "environment you do want to make sure",
    "duration": 4.48,
    "offset": 39.84,
    "lang": "en"
  },
  {
    "text": "that you&amp;#39;re using a gpu for the hardware",
    "duration": 4.56,
    "offset": 41.92,
    "lang": "en"
  },
  {
    "text": "accelerator that&amp;#39;ll help it to go",
    "duration": 5.68,
    "offset": 44.32,
    "lang": "en"
  },
  {
    "text": "faster and i&amp;#39;m going to go ahead and",
    "duration": 5.52,
    "offset": 46.48,
    "lang": "en"
  },
  {
    "text": "begin and show you",
    "duration": 3.84,
    "offset": 50,
    "lang": "en"
  },
  {
    "text": "the different parts as we run this i&amp;#39;m",
    "duration": 3.36,
    "offset": 52,
    "lang": "en"
  },
  {
    "text": "going to run this we&amp;#39;re going to use",
    "duration": 5.6,
    "offset": 53.84,
    "lang": "en"
  },
  {
    "text": "tensorflow 2.x and that&amp;#39;s that is ready",
    "duration": 6.64,
    "offset": 55.36,
    "lang": "en"
  },
  {
    "text": "we&amp;#39;re going to go ahead and install",
    "duration": 5.84,
    "offset": 59.44,
    "lang": "en"
  },
  {
    "text": "hugging face it&amp;#39;s not included in collab",
    "duration": 4.64,
    "offset": 62,
    "lang": "en"
  },
  {
    "text": "by default",
    "duration": 2.8,
    "offset": 65.28,
    "lang": "en"
  },
  {
    "text": "this goes pretty quick but i&amp;#39;ll still",
    "duration": 3.6,
    "offset": 66.64,
    "lang": "en"
  },
  {
    "text": "fast forward through it and i&amp;#39;m going to",
    "duration": 4.64,
    "offset": 68.08,
    "lang": "en"
  },
  {
    "text": "load the emotion data set we&amp;#39;ve dealt",
    "duration": 5.12,
    "offset": 70.24,
    "lang": "en"
  },
  {
    "text": "with this one before it is",
    "duration": 5.28,
    "offset": 72.72,
    "lang": "en"
  },
  {
    "text": "tweets and they&amp;#39;ve been labeled to six",
    "duration": 5.68,
    "offset": 75.36,
    "lang": "en"
  },
  {
    "text": "different emotion types you can see what",
    "duration": 4.88,
    "offset": 78,
    "lang": "en"
  },
  {
    "text": "an individual tweet looks like and you",
    "duration": 4.399,
    "offset": 81.04,
    "lang": "en"
  },
  {
    "text": "can see the label is three",
    "duration": 7.04,
    "offset": 82.88,
    "lang": "en"
  },
  {
    "text": "to see the actual features that",
    "duration": 6.881,
    "offset": 85.439,
    "lang": "en"
  },
  {
    "text": "are labels really i i would prefer to",
    "duration": 4.4,
    "offset": 89.92,
    "lang": "en"
  },
  {
    "text": "call these labels and features but these",
    "duration": 4.08,
    "offset": 92.32,
    "lang": "en"
  },
  {
    "text": "are the",
    "duration": 4.08,
    "offset": 94.32,
    "lang": "en"
  },
  {
    "text": "the different emotion types now",
    "duration": 4.16,
    "offset": 96.4,
    "lang": "en"
  },
  {
    "text": "considering this is twitter",
    "duration": 5.92,
    "offset": 98.4,
    "lang": "en"
  },
  {
    "text": "i am surprised that trolling and",
    "duration": 7.199,
    "offset": 100.56,
    "lang": "en"
  },
  {
    "text": "political fervor are not among them i",
    "duration": 5.759,
    "offset": 104.32,
    "lang": "en"
  },
  {
    "text": "don&amp;#39;t even maybe both of those are",
    "duration": 5.441,
    "offset": 107.759,
    "lang": "en"
  },
  {
    "text": "special cases of anger but so now we&amp;#39;ll",
    "duration": 5.921,
    "offset": 110.079,
    "lang": "en"
  },
  {
    "text": "go ahead and load the auto tokenizer",
    "duration": 5.68,
    "offset": 113.2,
    "lang": "en"
  },
  {
    "text": "which we&amp;#39;re going to use the distiller",
    "duration": 4.079,
    "offset": 116,
    "lang": "en"
  },
  {
    "text": "base",
    "duration": 3.839,
    "offset": 118.88,
    "lang": "en"
  },
  {
    "text": "and this is going to break",
    "duration": 6,
    "offset": 120.079,
    "lang": "en"
  },
  {
    "text": "the tweets into sub word tokens we&amp;#39;ll",
    "duration": 5.121,
    "offset": 122.719,
    "lang": "en"
  },
  {
    "text": "see that in a moment it takes it a",
    "duration": 3.921,
    "offset": 126.079,
    "lang": "en"
  },
  {
    "text": "moment till it&amp;#39;s already loaded so that",
    "duration": 5.6,
    "offset": 127.84,
    "lang": "en"
  },
  {
    "text": "is good so now that we have",
    "duration": 5.92,
    "offset": 130,
    "lang": "en"
  },
  {
    "text": "all of this",
    "duration": 5.84,
    "offset": 133.44,
    "lang": "en"
  },
  {
    "text": "the the tokenizer loaded the data loaded",
    "duration": 5.92,
    "offset": 135.92,
    "lang": "en"
  },
  {
    "text": "we need to transform this into",
    "duration": 4.319,
    "offset": 139.28,
    "lang": "en"
  },
  {
    "text": "tensorflow",
    "duration": 3.44,
    "offset": 141.84,
    "lang": "en"
  },
  {
    "text": "type data so that it can be used to",
    "duration": 3.601,
    "offset": 143.599,
    "lang": "en"
  },
  {
    "text": "train the tensorflow",
    "duration": 3.52,
    "offset": 145.28,
    "lang": "en"
  },
  {
    "text": "neural network that we loaded now",
    "duration": 3.28,
    "offset": 147.2,
    "lang": "en"
  },
  {
    "text": "hugging face does allow you to pull",
    "duration": 3.519,
    "offset": 148.8,
    "lang": "en"
  },
  {
    "text": "those neural networks down",
    "duration": 2.88,
    "offset": 150.48,
    "lang": "en"
  },
  {
    "text": "in",
    "duration": 4.241,
    "offset": 152.319,
    "lang": "en"
  },
  {
    "text": "pi torch as well as tensorflow cara&amp;#39;s so",
    "duration": 4.879,
    "offset": 153.36,
    "lang": "en"
  },
  {
    "text": "you can you can use it with whichever",
    "duration": 3.52,
    "offset": 156.56,
    "lang": "en"
  },
  {
    "text": "one works the best",
    "duration": 4.561,
    "offset": 158.239,
    "lang": "en"
  },
  {
    "text": "i&amp;#39;m using the data collater which is",
    "duration": 5.92,
    "offset": 160.08,
    "lang": "en"
  },
  {
    "text": "going to be used to take the python",
    "duration": 6.24,
    "offset": 162.8,
    "lang": "en"
  },
  {
    "text": "dictionaries and the the hugging face",
    "duration": 5.2,
    "offset": 166,
    "lang": "en"
  },
  {
    "text": "format that they put",
    "duration": 4.559,
    "offset": 169.04,
    "lang": "en"
  },
  {
    "text": "the data in and it&amp;#39;s going to output it",
    "duration": 4.48,
    "offset": 171.2,
    "lang": "en"
  },
  {
    "text": "as tensorflow we&amp;#39;ll go ahead and run",
    "duration": 2.881,
    "offset": 173.599,
    "lang": "en"
  },
  {
    "text": "that",
    "duration": 2.08,
    "offset": 175.68,
    "lang": "en"
  },
  {
    "text": "this",
    "duration": 2.96,
    "offset": 176.48,
    "lang": "en"
  },
  {
    "text": "does not take long so i don&amp;#39;t need to",
    "duration": 4.24,
    "offset": 177.76,
    "lang": "en"
  },
  {
    "text": "fast forward it then we&amp;#39;re going to take",
    "duration": 3.6,
    "offset": 179.44,
    "lang": "en"
  },
  {
    "text": "and we&amp;#39;re going to",
    "duration": 4.08,
    "offset": 182,
    "lang": "en"
  },
  {
    "text": "get the training data set and the",
    "duration": 5.44,
    "offset": 183.04,
    "lang": "en"
  },
  {
    "text": "evaluation data set",
    "duration": 4,
    "offset": 186.08,
    "lang": "en"
  },
  {
    "text": "run those",
    "duration": 4.88,
    "offset": 188.48,
    "lang": "en"
  },
  {
    "text": "using the the the tokenized form of it",
    "duration": 5.76,
    "offset": 190.08,
    "lang": "en"
  },
  {
    "text": "that we that we previously created that",
    "duration": 4.799,
    "offset": 193.36,
    "lang": "en"
  },
  {
    "text": "was done up here i kind of jumped",
    "duration": 4.479,
    "offset": 195.84,
    "lang": "en"
  },
  {
    "text": "through that fairly quickly because we",
    "duration": 4.961,
    "offset": 198.159,
    "lang": "en"
  },
  {
    "text": "we&amp;#39;ve done this before in previous",
    "duration": 4.48,
    "offset": 200.319,
    "lang": "en"
  },
  {
    "text": "class modules but",
    "duration": 4.8,
    "offset": 203.12,
    "lang": "en"
  },
  {
    "text": "we we basically took all of the data",
    "duration": 4.881,
    "offset": 204.799,
    "lang": "en"
  },
  {
    "text": "broke into the sub words with the",
    "duration": 4.239,
    "offset": 207.92,
    "lang": "en"
  },
  {
    "text": "beginning and ending tokens and put it",
    "duration": 4.8,
    "offset": 209.68,
    "lang": "en"
  },
  {
    "text": "put it all together so now we can break",
    "duration": 4.961,
    "offset": 212.159,
    "lang": "en"
  },
  {
    "text": "that into the training and the",
    "duration": 6.64,
    "offset": 214.48,
    "lang": "en"
  },
  {
    "text": "evaluation sets as is typical for",
    "duration": 6.679,
    "offset": 217.12,
    "lang": "en"
  },
  {
    "text": "hugging face data sets they&amp;#39;re already",
    "duration": 6.08,
    "offset": 221.12,
    "lang": "en"
  },
  {
    "text": "pre-broken into train and evaluation for",
    "duration": 4.281,
    "offset": 223.799,
    "lang": "en"
  },
  {
    "text": "you",
    "duration": 3.44,
    "offset": 227.2,
    "lang": "en"
  },
  {
    "text": "now i&amp;#39;m going to build the two",
    "duration": 6.159,
    "offset": 228.08,
    "lang": "en"
  },
  {
    "text": "tensorflow data sets and i&amp;#39;m passing in",
    "duration": 5.679,
    "offset": 230.64,
    "lang": "en"
  },
  {
    "text": "the columns that we&amp;#39;re extracting so",
    "duration": 3.92,
    "offset": 234.239,
    "lang": "en"
  },
  {
    "text": "we&amp;#39;re getting the attention masks that&amp;#39;s",
    "duration": 2.881,
    "offset": 236.319,
    "lang": "en"
  },
  {
    "text": "telling",
    "duration": 3.201,
    "offset": 238.159,
    "lang": "en"
  },
  {
    "text": "which of the",
    "duration": 4.56,
    "offset": 239.2,
    "lang": "en"
  },
  {
    "text": "parts of the sequence are not padding",
    "duration": 4.4,
    "offset": 241.36,
    "lang": "en"
  },
  {
    "text": "and need to be paid attention to the",
    "duration": 5.199,
    "offset": 243.76,
    "lang": "en"
  },
  {
    "text": "input ids which tells us",
    "duration": 6.64,
    "offset": 245.76,
    "lang": "en"
  },
  {
    "text": "that the tokens that each of the these",
    "duration": 5.761,
    "offset": 248.959,
    "lang": "en"
  },
  {
    "text": "sub words were broken into",
    "duration": 4.239,
    "offset": 252.4,
    "lang": "en"
  },
  {
    "text": "and then the",
    "duration": 4.88,
    "offset": 254.72,
    "lang": "en"
  },
  {
    "text": "labels that it&amp;#39;s going to align to batch",
    "duration": 5.521,
    "offset": 256.639,
    "lang": "en"
  },
  {
    "text": "size is eight so we have a full",
    "duration": 4.96,
    "offset": 259.6,
    "lang": "en"
  },
  {
    "text": "supervised training set there",
    "duration": 4.08,
    "offset": 262.16,
    "lang": "en"
  },
  {
    "text": "now we&amp;#39;re ready to",
    "duration": 4.4,
    "offset": 264.56,
    "lang": "en"
  },
  {
    "text": "download the",
    "duration": 6.959,
    "offset": 266.24,
    "lang": "en"
  },
  {
    "text": "we&amp;#39;re using distiller again and this is",
    "duration": 5.44,
    "offset": 268.96,
    "lang": "en"
  },
  {
    "text": "using the",
    "duration": 3.28,
    "offset": 273.199,
    "lang": "en"
  },
  {
    "text": "the model for sequence classification so",
    "duration": 4.72,
    "offset": 274.4,
    "lang": "en"
  },
  {
    "text": "we&amp;#39;re going to classify those six",
    "duration": 6.321,
    "offset": 276.479,
    "lang": "en"
  },
  {
    "text": "labels now we can compile and fit",
    "duration": 4.639,
    "offset": 279.12,
    "lang": "en"
  },
  {
    "text": "the",
    "duration": 3.04,
    "offset": 282.8,
    "lang": "en"
  },
  {
    "text": "neural network now it&amp;#39;s going to go",
    "duration": 4,
    "offset": 283.759,
    "lang": "en"
  },
  {
    "text": "through these epochs for a while and",
    "duration": 4.48,
    "offset": 285.84,
    "lang": "en"
  },
  {
    "text": "you&amp;#39;re going to see the log loss the",
    "duration": 5.281,
    "offset": 287.759,
    "lang": "en"
  },
  {
    "text": "loss function slowly decrease as it goes",
    "duration": 4.319,
    "offset": 290.32,
    "lang": "en"
  },
  {
    "text": "through all of these you certainly want",
    "duration": 3.84,
    "offset": 293.04,
    "lang": "en"
  },
  {
    "text": "to make sure you have your gpu enabled",
    "duration": 4.801,
    "offset": 294.639,
    "lang": "en"
  },
  {
    "text": "so that these don&amp;#39;t take completely",
    "duration": 4.159,
    "offset": 296.88,
    "lang": "en"
  },
  {
    "text": "forever",
    "duration": 3.52,
    "offset": 299.44,
    "lang": "en"
  },
  {
    "text": "you can see the sparse categorical",
    "duration": 4.641,
    "offset": 301.039,
    "lang": "en"
  },
  {
    "text": "accuracy is increasing",
    "duration": 5.92,
    "offset": 302.96,
    "lang": "en"
  },
  {
    "text": "by the way what categoric what sparse",
    "duration": 5.2,
    "offset": 305.68,
    "lang": "en"
  },
  {
    "text": "accuracy means is we&amp;#39;re not putting them",
    "duration": 4.96,
    "offset": 308.88,
    "lang": "en"
  },
  {
    "text": "into one hot encoding where this the",
    "duration": 5.12,
    "offset": 310.88,
    "lang": "en"
  },
  {
    "text": "six emotions would each have separate",
    "duration": 4.16,
    "offset": 313.84,
    "lang": "en"
  },
  {
    "text": "columns with a one and just one and",
    "duration": 3.44,
    "offset": 316,
    "lang": "en"
  },
  {
    "text": "zeros in the other",
    "duration": 3.44,
    "offset": 318,
    "lang": "en"
  },
  {
    "text": "rather we&amp;#39;re actually passing in the",
    "duration": 4.64,
    "offset": 319.44,
    "lang": "en"
  },
  {
    "text": "index so one two three zero one two",
    "duration": 4.72,
    "offset": 321.44,
    "lang": "en"
  },
  {
    "text": "three whatever the",
    "duration": 5.2,
    "offset": 324.08,
    "lang": "en"
  },
  {
    "text": "the actual label indices were",
    "duration": 5.2,
    "offset": 326.16,
    "lang": "en"
  },
  {
    "text": "so this is going to continue i set it to",
    "duration": 5.04,
    "offset": 329.28,
    "lang": "en"
  },
  {
    "text": "train for five epochs and that gets it",
    "duration": 6.16,
    "offset": 331.36,
    "lang": "en"
  },
  {
    "text": "to a reasonably decent accuracy",
    "duration": 6.159,
    "offset": 334.32,
    "lang": "en"
  },
  {
    "text": "level so this is something you can do",
    "duration": 5.76,
    "offset": 337.52,
    "lang": "en"
  },
  {
    "text": "to fine-tune and train for your own",
    "duration": 5.361,
    "offset": 340.479,
    "lang": "en"
  },
  {
    "text": "neural networks you can then borrow",
    "duration": 5.04,
    "offset": 343.28,
    "lang": "en"
  },
  {
    "text": "the the burnt models that were already",
    "duration": 4.88,
    "offset": 345.84,
    "lang": "en"
  },
  {
    "text": "created and trained for",
    "duration": 4.159,
    "offset": 348.32,
    "lang": "en"
  },
  {
    "text": "from google so you&amp;#39;re you&amp;#39;re basically",
    "duration": 3.52,
    "offset": 350.72,
    "lang": "en"
  },
  {
    "text": "just taking",
    "duration": 3.28,
    "offset": 352.479,
    "lang": "en"
  },
  {
    "text": "google or other companies really but",
    "duration": 3.519,
    "offset": 354.24,
    "lang": "en"
  },
  {
    "text": "you&amp;#39;re just taking those",
    "duration": 5.041,
    "offset": 355.759,
    "lang": "en"
  },
  {
    "text": "pre-trained weights and then adapting",
    "duration": 5.361,
    "offset": 357.759,
    "lang": "en"
  },
  {
    "text": "those transfer learning style",
    "duration": 4.399,
    "offset": 360.8,
    "lang": "en"
  },
  {
    "text": "to whatever it is you&amp;#39;re trying to",
    "duration": 3.6,
    "offset": 363.12,
    "lang": "en"
  },
  {
    "text": "actually train for thank you for",
    "duration": 4.321,
    "offset": 365.199,
    "lang": "en"
  },
  {
    "text": "watching this video and if you&amp;#39;d like to",
    "duration": 4.16,
    "offset": 366.72,
    "lang": "en"
  },
  {
    "text": "keep up with this course please",
    "duration": 3.28,
    "offset": 369.52,
    "lang": "en"
  },
  {
    "text": "subscribe to my youtube channel so that",
    "duration": 3.759,
    "offset": 370.88,
    "lang": "en"
  },
  {
    "text": "you can see all my latest videos on",
    "duration": 3.839,
    "offset": 372.8,
    "lang": "en"
  },
  {
    "text": "artificial intelligence and natural",
    "duration": 4.881,
    "offset": 374.639,
    "lang": "en"
  },
  {
    "text": "language processing",
    "duration": 2.881,
    "offset": 376.639,
    "lang": "en"
  }
]