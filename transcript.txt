hi this is jeffy and welcome to applications of deep neural networks with washington university in this video we're going to see how we can actually train a hugging face neural network fine-tune it further from the pre-trained weights that it has using a data set on hugging face as well as a pre-trained model on hugging face [Music] so now we're going to actually train a model and hugging face i'm going to go ahead and open this in collab pro you don't really need colab pro i'm using co-ed pro plus it works just fine in the free environment you do want to make sure that you're using a gpu for the hardware accelerator that'll help it to go faster and i'm going to go ahead and begin and show you the different parts as we run this i'm going to run this we're going to use tensorflow 2.x and that's that is ready we're going to go ahead and install hugging face it's not included in collab by default this goes pretty quick but i'll still fast forward through it and i'm going to load the emotion data set we've dealt with this one before it is tweets and they've been labeled to six different emotion types you can see what an individual tweet looks like and you can see the label is three to see the actual features that are labels really i i would prefer to call these labels and features but these are the the different emotion types now considering this is twitter i am surprised that trolling and political fervor are not among them i don't even maybe both of those are special cases of anger but so now we'll go ahead and load the auto tokenizer which we're going to use the distiller base and this is going to break the tweets into sub word tokens we'll see that in a moment it takes it a moment till it's already loaded so that is good so now that we have all of this the the tokenizer loaded the data loaded we need to transform this into tensorflow type data so that it can be used to train the tensorflow neural network that we loaded now hugging face does allow you to pull those neural networks down in pi torch as well as tensorflow cara's so you can you can use it with whichever one works the best i'm using the data collater which is going to be used to take the python dictionaries and the the hugging face format that they put the data in and it's going to output it as tensorflow we'll go ahead and run that this does not take long so i don't need to fast forward it then we're going to take and we're going to get the training data set and the evaluation data set run those using the the the tokenized form of it that we that we previously created that was done up here i kind of jumped through that fairly quickly because we we've done this before in previous class modules but we we basically took all of the data broke into the sub words with the beginning and ending tokens and put it put it all together so now we can break that into the training and the evaluation sets as is typical for hugging face data sets they're already pre-broken into train and evaluation for you now i'm going to build the two tensorflow data sets and i'm passing in the columns that we're extracting so we're getting the attention masks that's telling which of the parts of the sequence are not padding and need to be paid attention to the input ids which tells us that the tokens that each of the these sub words were broken into and then the labels that it's going to align to batch size is eight so we have a full supervised training set there now we're ready to download the we're using distiller again and this is using the the model for sequence classification so we're going to classify those six labels now we can compile and fit the neural network now it's going to go through these epochs for a while and you're going to see the log loss the loss function slowly decrease as it goes through all of these you certainly want to make sure you have your gpu enabled so that these don't take completely forever you can see the sparse categorical accuracy is increasing by the way what categoric what sparse accuracy means is we're not putting them into one hot encoding where this the six emotions would each have separate columns with a one and just one and zeros in the other rather we're actually passing in the index so one two three zero one two three whatever the the actual label indices were so this is going to continue i set it to train for five epochs and that gets it to a reasonably decent accuracy level so this is something you can do to fine-tune and train for your own neural networks you can then borrow the the burnt models that were already created and trained for from google so you're you're basically just taking google or other companies really but you're just taking those pre-trained weights and then adapting those transfer learning style to whatever it is you're trying to actually train for thank you for watching this video and if you'd like to keep up with this course please subscribe to my youtube channel so that you can see all my latest videos on artificial intelligence and natural language processing